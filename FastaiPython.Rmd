---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
library(reticulate)
use_virtualenv("r-reticulate")
```


```{python}
#All needed functions that are depricated in the latest version of fastai
#Functions from 2019 tutorial
from enum import IntEnum
#Function to open our images
def open_image(fn):
    """ Opens an image using OpenCV given the file path.

    Arguments:
        fn: the file path of the image

    Returns:
        The image in RGB format as numpy array of floats normalized to range between 0.0 - 1.0
    """
    flags = cv2.IMREAD_UNCHANGED+cv2.IMREAD_ANYDEPTH+cv2.IMREAD_ANYCOLOR
    if not os.path.exists(fn):
        raise OSError('No such file or directory: {}'.format(fn))
    elif os.path.isdir(fn):
        raise OSError('Is a directory: {}'.format(fn))
    else:
        try:
            if str(fn).startswith("http"):
                req = urllib.urlopen(str(fn))
                image = np.asarray(bytearray(resp.read()), dtype="uint8")
                im = cv2.imdecode(image, flags).astype(np.float32)/255
            else:
                im = cv2.imread(str(fn), flags).astype(np.float32)/255
            if im is None: raise OSError(f'File not recognized by opencv: {fn}')
            return cv2.cvtColor(im, cv2.COLOR_BGR2RGB)
        except Exception as e:
            raise OSError('Error handling image at: {}'.format(fn)) from e
          
class CropType(IntEnum):
    """ Type of image cropping. """
    RANDOM = 1
    CENTER = 2
    NO = 3
    GOOGLENET = 4
    
class TfmType(IntEnum):
    """ Type of transformation.
    Parameters
        IntEnum: predefined types of transformations
            NO:    the default, y does not get transformed when x is transformed.
            PIXEL: x and y are images and should be transformed in the same way.
                   Example: image segmentation.
            COORD: y are coordinates (i.e bounding boxes)
            CLASS: y are class labels (same behaviour as PIXEL, except no normalization)
    """
    NO = 1
    PIXEL = 2
    COORD = 3
    CLASS = 4

class RandomLighting(Transform):
    def __init__(self, b, c, tfm_y=TfmType.NO):
        super().__init__(tfm_y)
        self.b,self.c = b,c

    def set_state(self):
        self.store.b_rand = rand0(self.b)
        self.store.c_rand = rand0(self.c)

    def do_transform(self, x, is_y):
        if is_y and self.tfm_y != TfmType.PIXEL: return x
        b = self.store.b_rand
        c = self.store.c_rand
        c = -1/(c-1) if c<0 else c+1
        x = lighting(x, b, c)
        return x

class CoordTransform(Transform):
    """ A coordinate transform.  """

    @staticmethod
    def make_square(y, x):
        r,c,*_ = x.shape
        y1 = np.zeros((r, c))
        y = y.astype(np.int)
        y1[y[0]:y[2], y[1]:y[3]] = 1.
        return y1

    def map_y(self, y0, x):
        y = CoordTransform.make_square(y0, x)
        y_tr = self.do_transform(y, True)
        return to_bb(y_tr)

    def transform_coord(self, x, ys):
        yp = partition(ys, 4)
        y2 = [self.map_y(y,x) for y in yp]
        x = self.do_transform(x, False)
        return x, np.concatenate(y2)


def tfms_from_stats(stats, sz, aug_tfms=None, max_zoom=None, pad=0, crop_type=CropType.RANDOM,
                    tfm_y=None, sz_y=None, pad_mode=cv2.BORDER_REFLECT, norm_y=True, scale=None):
    """ Given the statistics of the training image sets, returns separate training and validation transform functions
    """
    if aug_tfms is None: aug_tfms=[]
    tfm_norm = Normalize(*stats, tfm_y=tfm_y if norm_y else TfmType.NO) if stats is not None else None
    tfm_denorm = Denormalize(*stats) if stats is not None else None
    val_crop = CropType.CENTER if crop_type in (CropType.RANDOM,CropType.GOOGLENET) else crop_type
    val_tfm = image_gen(tfm_norm, tfm_denorm, sz, pad=pad, crop_type=val_crop,
            tfm_y=tfm_y, sz_y=sz_y, scale=scale)
    trn_tfm = image_gen(tfm_norm, tfm_denorm, sz, pad=pad, crop_type=crop_type,
            tfm_y=tfm_y, sz_y=sz_y, tfms=aug_tfms, max_zoom=max_zoom, pad_mode=pad_mode, scale=scale)
    return trn_tfm, val_tfm

def tfms_from_model(f_model, sz, aug_tfms=None, max_zoom=None, pad=0, crop_type=CropType.RANDOM,
                    tfm_y=None, sz_y=None, pad_mode=cv2.BORDER_REFLECT, norm_y=True, scale=None):
    """ Returns separate transformers of images for training and validation.
    Transformers are constructed according to the image statistics given by the model. (See tfms_from_stats)
    Arguments:
        f_model: model, pretrained or not pretrained
    """
    stats = inception_stats if f_model in inception_models else imagenet_stats
    return tfms_from_stats(stats, sz, aug_tfms, max_zoom=max_zoom, pad=pad, crop_type=crop_type,
                           tfm_y=tfm_y, sz_y=sz_y, pad_mode=pad_mode, norm_y=norm_y, scale=scale)


class RandomRotate(CoordTransform):
    """ Rotates images and (optionally) target y.
    Rotating coordinates is treated differently for x and y on this
    transform.
     Arguments:
        deg (float): degree to rotate.
        p (float): probability of rotation
        mode: type of border
        tfm_y (TfmType): type of y transform
    """
    def __init__(self, deg, p=0.75, mode=cv2.BORDER_REFLECT, tfm_y=TfmType.NO):
        super().__init__(tfm_y)
        self.deg,self.p = deg,p
        if tfm_y == TfmType.COORD or tfm_y == TfmType.CLASS:
            self.modes = (mode,cv2.BORDER_CONSTANT)
        else:
            self.modes = (mode,mode)

    def set_state(self):
        self.store.rdeg = rand0(self.deg)
        self.store.rp = random.random()<self.p

    def do_transform(self, x, is_y):
        if self.store.rp: x = rotate_cv(x, self.store.rdeg, 
                mode= self.modes[1] if is_y else self.modes[0],
                interpolation=cv2.INTER_NEAREST if is_y else cv2.INTER_AREA)
        return x

class RandomFlip(CoordTransform):
    def __init__(self, tfm_y=TfmType.NO, p=0.5):
        super().__init__(tfm_y)
        self.p=p

    def set_state(self): self.store.do_flip = random.random()<self.p

    def do_transform(self, x, is_y): return np.fliplr(x).copy() if self.store.do_flip else x

class RandomLighting(Transform):
    def __init__(self, b, c, tfm_y=TfmType.NO):
        super().__init__(tfm_y)
        self.b,self.c = b,c

    def set_state(self):
        self.store.b_rand = rand0(self.b)
        self.store.c_rand = rand0(self.c)

    def do_transform(self, x, is_y):
        if is_y and self.tfm_y != TfmType.PIXEL: return x
        b = self.store.b_rand
        c = self.store.c_rand
        c = -1/(c-1) if c<0 else c+1
        x = lighting(x, b, c)
        return x

transforms_basic = [RandomRotate(10), RandomLighting(0.05, 0.05)]
transforms_side_on  = transforms_basic + [RandomFlip()]

inception_stats = ([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])
inception_models = (inception_4, inceptionresnet_2)
          

```

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import json
from fastai import *
from fastai.tr import *
from pathlib import Path
from PIL import ImageDraw, ImageFont
from matplotlib import patches, patheffects
from fastai.learner import *
from fastai.data import *
from collections import defaultdict
import collections
import cv2
import os
from fastai.vision.all import *


PATH = Path('data/shoes')
list(PATH.iterdir())
IMG_PATH = Path('shoes_num')
shoe_pics = os.listdir(IMG_PATH)
trn_j = json.load((PATH /'shoe_textures.json').open())
trn_j.keys()

IMAGES, ANNOTATIONS, CATEGORIES = ['images', 'annotations', 'categories']
trn_j[IMAGES][:1]
trn_j[ANNOTATIONS][:2]
trn_j[CATEGORIES][:4]

FILE_NAME, ID, IMG_ID, CAT_ID, BBOX = 'file_name', 'id', 'image_id', 'category_id', 'bbox'
cats = dict((o[ID], o['name']) for o in trn_j[CATEGORIES])
trn_fns = dict((o[ID], o[FILE_NAME]) for o in trn_j[IMAGES])
trn_ids = [o[ID] for o in trn_j[IMAGES]]

im0_d = trn_j[IMAGES][0]
im0_d[FILE_NAME], im0_d[ID]

trn_anno = collections.defaultdict(lambda:[])
for o in trn_j[ANNOTATIONS]:
    if not o['ignore']:
        bb = o[BBOX]  # one bbox. looks like '[155, 96, 196, 174]'.
        bb = np.array([bb[1], bb[0], bb[3] + bb[1]- 1, bb[2] + bb[0] - 1 ]) # output '[96 155 269 350]'.
        trn_anno[o[IMG_ID]].append((bb, o[CAT_ID]))

len(trn_anno)

list(trn_anno.values())[0]

im_a0 = trn_anno[im0_d[ID]]
im_a0

im_a0 = im_a[0]
im_a0

cats[6]

trn_anno[17]

#Some libraries take bounding boxes opposite of what we just did
def bb_hw(a):
    return np.array([ a[1], a[0], a[3] - a[1] + 1, a[2] - a[0] + 1 ])

          
im = open_image("shoes_num/1.jpg")

#Plot.subplots is super great about making our plots versatile
def show_img(im, figsize = None, ax = None):
  if not ax: fix,ax = plt.subplots(figsize=figsize)
  ax.imshow(im)
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)
  return ax

#Setting a appropriate colors for bounding boxes
def draw_outline(o, lw):
    o.set_path_effects([patheffects.Stroke(linewidth=lw, foreground='black'),
                          patheffects.Normal()])
                          
#Drawing the actual bounding boxes
def draw_rect(ax, b):
  patch = ax.add_patch(patches.Rectangle(b[:2], *b[-2:], fill = False, edgecolor = 'white', lw = 2))
  draw_outline(patch, 4)
  
#Creating the text of the appropraite class ax is axis object
def draw_text(ax, xy, txt, sz=14):
  text = ax.text(*xy, txt,
  verticalalignment = 'top', color='white', fontsize = sz, weight='bold')
  draw_outline(text, 1)
  
#Let's try it!!
ax = show_img(im)
b = bb_hw(im_a[0])
draw_rect(ax, b)
draw_text(ax, b[:2], cats[im_a[1]])
plt.show()
#Inserting the https//www. part
x = 0
arr = []
#key,value
for x in trn_fns.keys():
  #arr[x = trn_fns[x][:8] + "www." + trn_fns[x][8:]
  #print(trn_fns[x])
  arr.append(trn_fns[x].split('//')[0] + "//www." + trn_fns[x].split('//')[1])
print(arr)
arr[1]

#packaging the entire thing together into a function
def draw_im(im, ann):
  ax = show_img(im, figsize=(16,8))
  for b,c in ann:
    b = bb_hw(b)
    draw_rect(ax, b)
    draw_text(ax, b[:2], cats[c], sz=16)

#Beautiful. Now can call the contents of any image using this function
def draw_idx(i):
  im_a = trn_anno[i]
  im = open_image(IMG_PATH/shoe_pics[i])
  print(im.shape)
  draw_im(im, im_a)
draw_idx(14)
plt.show()
##Classifier is done! Now find largest item classifier
#------------------------------------------------------------
def get_lrg(b):
    if not b:
        raise Exception()
    # x is tuple. e.g.: (array([96 155 269 350]), 16)
    # x[0] returns a numpy array. e.g.: [96 155 269 350]
    # x[0][-2:] returns a numpy array. e.g.: [269 350]. This is the width x height of a bbox.
    # x[0][:2] returns a numpy array. e.g.: [96 155]. This is the x/y coord of a bbox.
    # np.product(x[0][-2:] - x[0][:2]) returns a scalar. e.g.: 33735
    b = sorted(b, key=lambda x: np.product(x[0][-2:] - x[0][:2]), reverse=True)
    return b[0] # get the first element in the list, which is the largest bbox for one image.

# a is image id (int), b is tuple of bbox (numpy array) & class id (int)
trn_lrg_anno = { a: get_lrg(b) for a, b in trn_anno.items()}

#Plot largest bounding box of an image
b,c = trn_lrg_anno[23]
b = bb_hw(b)
ax = show_img(open_image(IMG_PATH/shoe_pics[23]), figsize=(5,10))
draw_rect(ax,b)
draw_text(ax, b[:2], cats[c], sz=16)
plt.show() #Wohooo!

#Create a csv of the data
PATH2 = Path('shoes_num')
(IMG_PATH/'tmp').mkdir(exist_ok=True)
CSV = IMG_PATH/'tmp/shoes.csv'

df = pd.DataFrame({'fn': [shoe_pics[o] for o in trn_ids],
                    'cat': [cats[trn_lrg_anno[o][1]] for o in trn_ids] }, columns=['fn', 'cat'])
df.to_csv(CSV, index=False)

#Fit a model--ugh
f_model = resnet34
sz = 224
bs = 64

learn = cnn_learner(data, models.resnet50, metrics=error_rate)
tfms = tfms_from_model(resnet34, sz)
tfms = tfms_from_model(f_model, sz, aug_tfms=transforms_side_on, crop_type=CropType.NO)
md = ImageClassifierData.from_csv(PATH, JPEGS, CSV, tfms=tfms, bs=bs)

```

```{r}
result <- fromJSON(file = 'shoe_textures.json')

cats <- list()
trn_fns <- list()
trn_ids <- list()

for(i in 1:length(result$categories)){
  cats[i] = result$categories[[i]]$name
}
for(i in 1:length(result$images)){
  trn_fns[i] = result$images[[i]]$file_name
}
for(i in 1:length(result$images)){
  trn_ids[i] = result$images[[i]]$id
}

for(i in 1:length(trn_fns)){
  trn_fns[[i]] <- stringr::str_replace(trn_fns[[i]], "https://", "https://www.")
}

```